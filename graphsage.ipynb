{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 32, 30)\n",
      "2880\n",
      "torch.Size([32, 30])\n",
      "loader tensor([[3.8687e-07, 4.6613e-07, 5.3260e-07, 1.4376e-07, 5.5562e-07, 6.8038e-07,\n",
      "         3.7298e-07, 3.8315e-07, 9.0654e-07, 9.3240e-07, 6.1481e-07, 3.4832e-07,\n",
      "         4.6013e-07, 3.6838e-07, 6.7272e-07, 4.7111e-07, 5.5869e-07, 8.8953e-07,\n",
      "         2.3090e-07, 8.8521e-07, 2.1437e-07, 4.8598e-07, 4.7125e-07, 2.9518e-07,\n",
      "         5.3914e-07, 6.3746e-07, 7.3941e-07, 9.1913e-07, 4.0791e-07, 2.3668e-07],\n",
      "        [1.0997e-06, 1.1365e-06, 1.0279e-06, 4.1245e-07, 9.6109e-07, 1.9613e-06,\n",
      "         1.9759e-06, 1.7818e-06, 3.5109e-06, 2.1051e-06, 2.2427e-06, 7.2981e-07,\n",
      "         5.6689e-07, 5.1236e-07, 2.8042e-06, 8.6886e-07, 1.2810e-06, 1.8242e-06,\n",
      "         1.0040e-06, 1.0020e-06, 8.0691e-07, 1.9977e-06, 1.4499e-06, 6.6204e-07,\n",
      "         8.7982e-07, 1.4272e-06, 1.2729e-06, 8.9667e-07, 5.4645e-07, 6.8404e-07],\n",
      "        [6.6198e-08, 1.0031e-07, 4.6065e-08, 4.6944e-08, 6.8400e-08, 1.1024e-07,\n",
      "         8.6374e-08, 5.8051e-08, 1.0431e-07, 9.8463e-08, 1.7253e-07, 7.7455e-08,\n",
      "         6.5222e-08, 6.5854e-08, 1.7581e-07, 1.3092e-07, 1.4808e-07, 7.6841e-08,\n",
      "         1.1788e-07, 1.5782e-07, 1.2266e-07, 1.1193e-07, 1.3121e-07, 1.2492e-07,\n",
      "         6.2090e-08, 1.1561e-07, 4.7151e-08, 2.1415e-07, 9.2997e-08, 8.5148e-08],\n",
      "        [6.4380e-08, 2.0151e-07, 5.4473e-08, 7.2536e-08, 1.6154e-07, 1.9782e-07,\n",
      "         2.0224e-07, 1.0580e-07, 1.9196e-07, 2.0879e-07, 1.9242e-07, 9.8660e-08,\n",
      "         1.5037e-07, 1.2502e-07, 2.4885e-07, 1.8612e-07, 1.5693e-07, 1.0575e-07,\n",
      "         2.2778e-07, 2.2106e-07, 9.5770e-08, 1.5848e-07, 2.7961e-07, 2.4136e-07,\n",
      "         8.5338e-08, 1.1221e-07, 1.6449e-07, 2.9906e-07, 1.0370e-07, 1.2110e-07],\n",
      "        [1.1711e-07, 1.7904e-07, 8.1067e-08, 1.4771e-07, 2.1399e-07, 3.7305e-07,\n",
      "         2.5747e-07, 2.0779e-07, 1.4146e-07, 1.9080e-07, 2.4716e-07, 1.0274e-07,\n",
      "         1.5517e-07, 1.3006e-07, 1.3858e-07, 1.6329e-07, 3.1158e-07, 3.6315e-07,\n",
      "         1.4588e-07, 2.1616e-07, 1.5994e-07, 1.8109e-07, 2.7199e-07, 1.5217e-07,\n",
      "         1.2863e-07, 2.5037e-07, 1.0479e-07, 2.7252e-07, 1.8560e-07, 1.3568e-07],\n",
      "        [9.8085e-08, 1.7367e-07, 1.9668e-07, 9.8657e-08, 1.3407e-07, 2.2905e-07,\n",
      "         2.6710e-07, 2.4317e-07, 8.3243e-08, 1.8519e-07, 1.9321e-07, 1.4813e-07,\n",
      "         1.8484e-07, 1.8355e-07, 2.5300e-07, 1.1121e-07, 3.2350e-07, 1.4728e-07,\n",
      "         2.7185e-07, 2.2748e-07, 1.2764e-07, 2.7078e-07, 3.4924e-07, 2.6738e-07,\n",
      "         8.8543e-08, 7.7333e-08, 1.3654e-07, 3.0751e-07, 1.5446e-07, 1.4914e-07],\n",
      "        [1.2144e-07, 2.6452e-07, 1.0568e-07, 2.9092e-07, 3.2437e-07, 4.6559e-07,\n",
      "         5.7367e-07, 1.9543e-07, 4.3503e-07, 2.4862e-07, 2.1249e-07, 1.5048e-07,\n",
      "         3.1612e-07, 2.5249e-07, 1.7823e-07, 1.3677e-07, 2.9431e-07, 2.8411e-07,\n",
      "         2.4912e-07, 2.8084e-07, 3.2969e-07, 4.2318e-07, 3.6493e-07, 2.2726e-07,\n",
      "         2.6458e-07, 1.5025e-07, 1.3624e-07, 2.3183e-07, 2.1294e-07, 2.2739e-07],\n",
      "        [3.3611e-08, 6.8176e-08, 2.5927e-08, 2.6032e-08, 4.3971e-08, 9.4991e-08,\n",
      "         9.4897e-08, 4.3073e-08, 8.9365e-08, 4.5744e-08, 8.2778e-08, 3.4424e-08,\n",
      "         4.4147e-08, 5.3619e-08, 8.8617e-08, 8.8467e-08, 6.8679e-08, 3.2608e-08,\n",
      "         9.9322e-08, 9.3110e-08, 4.9198e-08, 6.2321e-08, 1.0298e-07, 7.3667e-08,\n",
      "         3.8571e-08, 3.6126e-08, 7.3882e-08, 9.5875e-08, 4.5614e-08, 6.5799e-08],\n",
      "        [5.8984e-08, 6.6059e-08, 1.8361e-08, 3.8462e-08, 9.5816e-08, 9.9726e-08,\n",
      "         5.9601e-08, 7.0212e-08, 6.3374e-08, 7.0218e-08, 8.5517e-08, 6.3166e-08,\n",
      "         4.4566e-08, 4.1930e-08, 1.0497e-07, 5.9513e-08, 8.7093e-08, 6.6065e-08,\n",
      "         8.9451e-08, 9.4789e-08, 1.0812e-07, 7.8217e-08, 1.3151e-07, 7.3108e-08,\n",
      "         5.2509e-08, 1.0875e-07, 4.3162e-08, 1.1882e-07, 7.0635e-08, 4.0946e-08],\n",
      "        [7.5589e-08, 1.0861e-07, 5.4627e-08, 5.7750e-08, 1.4483e-07, 1.3521e-07,\n",
      "         2.5777e-07, 6.7613e-08, 9.6055e-08, 1.1194e-07, 1.1884e-07, 5.5322e-08,\n",
      "         8.3836e-08, 6.3959e-08, 1.6652e-07, 9.2221e-08, 2.1050e-07, 7.1767e-08,\n",
      "         1.9918e-07, 1.6641e-07, 8.3398e-08, 1.6338e-07, 2.2087e-07, 1.5292e-07,\n",
      "         4.0730e-08, 1.0602e-07, 7.6684e-08, 1.8584e-07, 6.9184e-08, 7.9538e-08],\n",
      "        [5.6181e-08, 8.9069e-08, 4.8562e-08, 7.4048e-08, 1.9078e-07, 2.0572e-07,\n",
      "         1.1613e-07, 1.5143e-07, 1.1661e-07, 7.4067e-08, 7.8874e-08, 4.6996e-08,\n",
      "         1.4255e-07, 1.0431e-07, 7.5597e-08, 7.9701e-08, 1.1546e-07, 1.5501e-07,\n",
      "         8.0125e-08, 1.1292e-07, 9.2155e-08, 1.1543e-07, 1.1477e-07, 8.5860e-08,\n",
      "         1.0007e-07, 1.0190e-07, 5.8394e-08, 1.6975e-07, 1.0338e-07, 7.6167e-08],\n",
      "        [2.7939e-08, 3.0936e-08, 1.1865e-08, 2.9227e-08, 4.0178e-08, 5.5389e-08,\n",
      "         3.3261e-08, 3.5162e-08, 3.4395e-08, 2.9528e-08, 7.3659e-08, 2.2144e-08,\n",
      "         2.1864e-08, 3.1827e-08, 7.3001e-08, 4.9491e-08, 3.1518e-08, 1.7444e-08,\n",
      "         5.4567e-08, 3.6587e-08, 5.5150e-08, 3.9226e-08, 7.0830e-08, 3.8925e-08,\n",
      "         2.8830e-08, 4.5080e-08, 5.7083e-08, 4.2212e-08, 3.5134e-08, 3.7224e-08],\n",
      "        [2.6268e-08, 3.6396e-08, 2.0418e-08, 1.4760e-08, 8.9670e-08, 1.1349e-07,\n",
      "         9.8037e-08, 1.8743e-08, 4.1223e-08, 4.1845e-08, 5.5558e-08, 2.1107e-08,\n",
      "         3.2833e-08, 2.8088e-08, 5.0047e-08, 5.1706e-08, 6.6571e-08, 3.5299e-08,\n",
      "         4.8712e-08, 7.3301e-08, 2.6961e-08, 5.3109e-08, 6.1019e-08, 4.8675e-08,\n",
      "         2.4402e-08, 2.8470e-08, 5.2115e-08, 7.3633e-08, 3.1448e-08, 3.6115e-08],\n",
      "        [1.9585e-08, 4.7540e-08, 3.3393e-08, 2.5628e-08, 1.3494e-07, 1.9214e-07,\n",
      "         3.8336e-08, 7.0536e-08, 5.0887e-08, 5.0894e-08, 6.7488e-08, 1.9756e-08,\n",
      "         3.5918e-08, 4.2367e-08, 5.9598e-08, 7.3509e-08, 5.4720e-08, 4.3149e-08,\n",
      "         5.0505e-08, 4.2391e-08, 4.9758e-08, 3.6310e-08, 5.4598e-08, 3.6372e-08,\n",
      "         5.1221e-08, 8.3303e-08, 2.3796e-08, 1.0030e-07, 6.0190e-08, 2.5557e-08],\n",
      "        [2.3559e-07, 1.6525e-07, 1.6612e-07, 1.5606e-07, 3.3014e-07, 3.3836e-07,\n",
      "         3.1674e-07, 2.4576e-07, 1.8925e-07, 2.5760e-07, 1.1489e-07, 7.6234e-08,\n",
      "         1.2550e-07, 1.3944e-07, 2.2948e-07, 7.9908e-08, 4.1488e-07, 1.6477e-07,\n",
      "         1.4051e-07, 1.6274e-07, 1.9177e-07, 2.9354e-07, 2.2504e-07, 1.5081e-07,\n",
      "         1.0580e-07, 2.3782e-07, 9.2640e-08, 3.7781e-07, 3.4486e-07, 1.0544e-07],\n",
      "        [5.7356e-08, 1.2982e-07, 7.8799e-08, 4.4617e-08, 2.2752e-07, 3.0848e-07,\n",
      "         8.1720e-08, 1.0569e-07, 1.6770e-07, 8.8074e-08, 9.9945e-08, 4.2838e-08,\n",
      "         9.4803e-08, 1.1687e-07, 7.5587e-08, 1.0032e-07, 1.1341e-07, 1.1281e-07,\n",
      "         5.3848e-08, 1.6767e-07, 9.6364e-08, 1.0461e-07, 1.1945e-07, 8.2822e-08,\n",
      "         1.5862e-07, 1.3747e-07, 1.2276e-07, 1.0909e-07, 8.6826e-08, 1.0095e-07],\n",
      "        [1.4399e-08, 1.2800e-08, 1.1684e-08, 2.4846e-08, 2.5624e-08, 4.2291e-08,\n",
      "         2.3953e-08, 1.7609e-08, 1.0520e-08, 2.5075e-08, 2.8217e-08, 6.5079e-09,\n",
      "         1.8644e-08, 2.3170e-08, 2.0446e-08, 1.9520e-08, 1.5421e-08, 2.2536e-08,\n",
      "         2.4475e-08, 1.5183e-08, 2.8170e-08, 2.3033e-08, 2.0115e-08, 1.6626e-08,\n",
      "         7.8156e-09, 2.5110e-08, 3.5318e-08, 2.7212e-08, 1.9800e-08, 2.2696e-08],\n",
      "        [1.3871e-08, 2.9919e-08, 1.9591e-08, 3.1730e-08, 2.3479e-08, 5.8859e-08,\n",
      "         2.9204e-08, 3.4859e-08, 2.9536e-08, 1.4640e-08, 3.5068e-08, 1.2467e-08,\n",
      "         2.6046e-08, 2.9548e-08, 2.9052e-08, 3.1699e-08, 2.2504e-08, 1.8175e-08,\n",
      "         2.9215e-08, 2.6682e-08, 2.6800e-08, 3.5830e-08, 3.2963e-08, 3.0748e-08,\n",
      "         1.8683e-08, 4.5541e-08, 2.6024e-08, 1.9949e-08, 2.0974e-08, 1.7723e-08],\n",
      "        [3.4638e-08, 5.0227e-08, 3.8491e-08, 1.9609e-08, 9.8812e-08, 1.9667e-07,\n",
      "         1.0898e-07, 4.3444e-08, 6.1676e-08, 4.5145e-08, 4.2539e-08, 2.5793e-08,\n",
      "         2.7551e-08, 2.6767e-08, 4.1266e-08, 4.5676e-08, 5.4510e-08, 3.6748e-08,\n",
      "         5.0613e-08, 8.4120e-08, 4.4522e-08, 7.4695e-08, 2.9924e-08, 3.3826e-08,\n",
      "         3.6400e-08, 6.4500e-08, 5.7938e-08, 9.3000e-08, 3.4221e-08, 2.3278e-08],\n",
      "        [2.7531e-08, 7.0546e-08, 5.1589e-08, 5.0961e-08, 1.5753e-07, 3.4309e-07,\n",
      "         1.6710e-07, 6.2434e-08, 4.8918e-08, 3.8676e-08, 9.3010e-08, 2.8928e-08,\n",
      "         5.2634e-08, 4.8854e-08, 9.9125e-08, 8.0191e-08, 4.2350e-08, 5.6236e-08,\n",
      "         5.1619e-08, 5.7028e-08, 6.5848e-08, 5.3196e-08, 1.1126e-07, 5.6629e-08,\n",
      "         6.9550e-08, 1.0352e-07, 5.7082e-08, 9.1128e-08, 4.6294e-08, 3.6231e-08],\n",
      "        [2.9703e-08, 2.4289e-08, 1.4419e-08, 2.8871e-08, 3.0319e-08, 3.5064e-08,\n",
      "         3.0762e-08, 1.6520e-08, 1.3221e-08, 2.3031e-08, 3.0376e-08, 2.0933e-08,\n",
      "         4.1452e-08, 3.3142e-08, 2.7506e-08, 1.9061e-08, 3.8878e-08, 3.4244e-08,\n",
      "         4.7467e-08, 3.2733e-08, 4.4677e-08, 5.1049e-08, 3.8144e-08, 3.9605e-08,\n",
      "         2.1769e-08, 4.2327e-08, 1.6063e-08, 3.9632e-08, 2.9361e-08, 1.9700e-08],\n",
      "        [7.6659e-08, 3.4829e-08, 3.1528e-08, 3.0611e-08, 8.1452e-08, 1.6431e-07,\n",
      "         8.1704e-08, 4.8586e-08, 6.1901e-08, 6.6030e-08, 4.2141e-08, 2.8010e-08,\n",
      "         7.1938e-08, 5.6747e-08, 6.4895e-08, 4.6044e-08, 7.7124e-08, 7.5887e-08,\n",
      "         6.1656e-08, 8.8300e-08, 6.4254e-08, 7.0995e-08, 5.9524e-08, 4.6752e-08,\n",
      "         5.5612e-08, 8.2510e-08, 4.6956e-08, 7.0140e-08, 7.2219e-08, 3.9022e-08],\n",
      "        [4.2802e-08, 8.6404e-08, 4.3776e-08, 6.4381e-08, 1.0091e-07, 1.7196e-07,\n",
      "         1.1216e-07, 3.2145e-08, 5.2216e-08, 5.4105e-08, 8.5692e-08, 3.3320e-08,\n",
      "         6.8182e-08, 4.6560e-08, 6.0919e-08, 6.2391e-08, 7.1340e-08, 7.5063e-08,\n",
      "         9.1683e-08, 6.2783e-08, 6.8120e-08, 1.2302e-07, 8.1465e-08, 5.4208e-08,\n",
      "         3.3179e-08, 9.0727e-08, 7.8933e-08, 1.0838e-07, 3.8774e-08, 3.7254e-08],\n",
      "        [1.2663e-07, 7.8481e-08, 8.3682e-08, 3.8939e-08, 1.3567e-07, 3.2441e-07,\n",
      "         1.6422e-07, 8.8424e-08, 1.0288e-07, 9.2376e-08, 7.2114e-08, 5.1021e-08,\n",
      "         1.3768e-07, 1.3545e-07, 1.5482e-07, 9.5060e-08, 1.3992e-07, 1.2620e-07,\n",
      "         8.9618e-08, 1.3469e-07, 7.8145e-08, 1.0641e-07, 1.2539e-07, 1.0236e-07,\n",
      "         1.0411e-07, 1.7801e-07, 6.5645e-08, 1.4366e-07, 1.2774e-07, 5.7074e-08],\n",
      "        [8.7304e-08, 1.5400e-07, 1.1666e-07, 9.7285e-08, 2.0583e-07, 3.5208e-07,\n",
      "         2.5358e-07, 1.0427e-07, 1.6992e-07, 2.0176e-07, 2.3120e-07, 7.5607e-08,\n",
      "         9.6611e-08, 1.4402e-07, 1.3272e-07, 1.0733e-07, 2.1570e-07, 1.7427e-07,\n",
      "         1.4881e-07, 2.2441e-07, 1.5947e-07, 2.2758e-07, 2.9849e-07, 1.7806e-07,\n",
      "         6.5788e-08, 1.0664e-07, 2.4642e-07, 2.6164e-07, 1.2427e-07, 7.2104e-08],\n",
      "        [1.5138e-07, 9.5979e-08, 9.4078e-08, 9.2674e-08, 1.5925e-07, 1.6653e-07,\n",
      "         1.3186e-07, 9.4065e-08, 1.3693e-07, 1.5730e-07, 1.1051e-07, 9.2633e-08,\n",
      "         1.8838e-07, 9.6124e-08, 2.4854e-07, 1.4513e-07, 2.5551e-07, 1.6493e-07,\n",
      "         1.4936e-07, 1.9539e-07, 1.6139e-07, 1.8975e-07, 2.1471e-07, 1.6257e-07,\n",
      "         1.6964e-07, 1.8192e-07, 4.9962e-08, 1.9720e-07, 1.6501e-07, 1.1822e-07],\n",
      "        [8.9849e-08, 2.0151e-07, 9.5018e-08, 9.5832e-08, 1.6710e-07, 2.1660e-07,\n",
      "         1.8986e-07, 8.1526e-08, 1.2907e-07, 1.6266e-07, 1.8015e-07, 8.0850e-08,\n",
      "         1.3996e-07, 1.1577e-07, 1.8974e-07, 1.4404e-07, 2.5247e-07, 1.4378e-07,\n",
      "         3.2039e-07, 1.5936e-07, 1.9806e-07, 3.2239e-07, 2.0711e-07, 1.3506e-07,\n",
      "         1.0053e-07, 8.3346e-08, 1.9193e-07, 2.8959e-07, 1.2504e-07, 9.1981e-08],\n",
      "        [4.6937e-07, 1.1408e-06, 4.5901e-07, 5.2777e-07, 8.9350e-07, 1.1391e-06,\n",
      "         1.0396e-06, 6.7030e-07, 8.0463e-07, 4.6988e-07, 8.5476e-07, 3.9795e-07,\n",
      "         7.1446e-07, 3.7568e-07, 1.4194e-06, 7.4755e-07, 1.5212e-06, 7.9998e-07,\n",
      "         1.6064e-06, 6.6077e-07, 8.2711e-07, 1.1542e-06, 1.2034e-06, 8.4595e-07,\n",
      "         9.0382e-07, 3.8920e-07, 4.9618e-07, 1.3649e-06, 5.1845e-07, 5.6813e-07],\n",
      "        [5.4936e-07, 1.3091e-06, 1.0602e-06, 7.9066e-07, 1.6992e-06, 1.3304e-06,\n",
      "         1.5693e-06, 1.1750e-06, 1.4572e-06, 6.7943e-07, 9.0394e-07, 4.6875e-07,\n",
      "         8.0072e-07, 4.9154e-07, 1.7798e-06, 1.0391e-06, 1.6163e-06, 7.5425e-07,\n",
      "         1.2046e-06, 1.2635e-06, 6.2191e-07, 1.1467e-06, 1.7395e-06, 1.2964e-06,\n",
      "         1.3793e-06, 1.1863e-06, 5.6419e-07, 1.9019e-06, 7.6736e-07, 8.0491e-07],\n",
      "        [6.6341e-07, 1.1737e-06, 5.8688e-07, 4.1670e-07, 8.1159e-07, 1.2673e-06,\n",
      "         1.1641e-06, 5.6381e-07, 8.7131e-07, 1.0037e-06, 1.0853e-06, 4.1502e-07,\n",
      "         7.1075e-07, 6.1596e-07, 9.2389e-07, 8.8069e-07, 1.5237e-06, 9.3505e-07,\n",
      "         1.8325e-06, 6.6987e-07, 1.1334e-06, 1.4509e-06, 1.6154e-06, 7.7657e-07,\n",
      "         5.6656e-07, 3.9962e-07, 9.8608e-07, 1.7834e-06, 6.2613e-07, 5.2911e-07],\n",
      "        [1.2433e-07, 2.4685e-07, 2.1742e-07, 1.3621e-07, 1.2195e-07, 2.2719e-07,\n",
      "         1.9713e-07, 1.9437e-07, 1.9950e-07, 1.7488e-07, 1.8244e-07, 8.2323e-08,\n",
      "         1.9894e-07, 1.0461e-07, 1.7748e-07, 1.1771e-07, 3.3676e-07, 2.1123e-07,\n",
      "         1.9158e-07, 2.9074e-07, 1.3138e-07, 2.2251e-07, 2.6301e-07, 2.2462e-07,\n",
      "         1.7765e-07, 1.2136e-07, 1.6829e-07, 1.9394e-07, 2.3556e-07, 1.2279e-07],\n",
      "        [2.5924e-07, 3.0835e-07, 3.2267e-07, 2.4457e-07, 2.5345e-07, 3.7148e-07,\n",
      "         4.1266e-07, 2.7471e-07, 2.1009e-07, 2.1733e-07, 3.4886e-07, 2.0843e-07,\n",
      "         2.2992e-07, 2.2111e-07, 1.1772e-07, 2.1869e-07, 4.5774e-07, 2.1766e-07,\n",
      "         3.9754e-07, 2.5028e-07, 2.8109e-07, 3.5036e-07, 4.8634e-07, 3.2861e-07,\n",
      "         2.5875e-07, 1.9767e-07, 1.5084e-07, 2.3553e-07, 2.0749e-07, 1.7673e-07]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclasclassen/Code/Master/AML_project_gnn/.venv/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "def load_graph_data():\n",
    "    # Load labels and features\n",
    "    y = np.load('label_valence_no_neutral_PSD_gamma.npy')\n",
    "    x = np.load('eeg_data_no_neutral_PSD_gamma.npy')\n",
    "    print(x.shape)\n",
    "    # Convert to PyTorch tensors\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    # fully connected graph for each graph\n",
    "    edge_index = list(itertools.combinations(range(32), 2))\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create a list of Data objects\n",
    "    data_list = [Data(x=x[i], edge_index=edge_index, y=y[i]) for i in range(x.shape[0])]\n",
    "    print(len(data_list))\n",
    "\n",
    "    print(data_list[0].x.shape)\n",
    "    return data_list\n",
    "\n",
    "# Load your graph data\n",
    "data_list = load_graph_data()\n",
    "# Create a DataLoader\n",
    "loader = DataLoader(data_list, batch_size=5, shuffle=True)\n",
    "print(\"loader\", loader.dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniclas-classen\u001b[0m (\u001b[33mniclasclassen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/niclasclassen/Code/Master/AML_project_gnn/wandb/run-20240504_170019-6gq2l61v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niclasclassen/aml_mini_project/runs/6gq2l61v' target=\"_blank\">dark-wookie-1</a></strong> to <a href='https://wandb.ai/niclasclassen/aml_mini_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/niclasclassen/aml_mini_project' target=\"_blank\">https://wandb.ai/niclasclassen/aml_mini_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/niclasclassen/aml_mini_project/runs/6gq2l61v' target=\"_blank\">https://wandb.ai/niclasclassen/aml_mini_project/runs/6gq2l61v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/niclasclassen/aml_mini_project/runs/6gq2l61v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15242c050>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"aml_mini_project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"GRAPHSage\",\n",
    "    \"dataset\": \"EEG\",\n",
    "    \"epochs\": 100,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.56111115 tensor(0.8000)\n",
      "1 0.56875 tensor(0.8000)\n",
      "2 0.56874996 tensor(0.4000)\n",
      "3 0.5687501 tensor(0.4000)\n",
      "4 0.56875 tensor(0.6000)\n",
      "5 0.56874996 tensor(0.6000)\n",
      "6 0.5687501 tensor(0.4000)\n",
      "7 0.5687501 tensor(0.6000)\n",
      "8 0.56875 tensor(0.4000)\n",
      "9 0.56875 tensor(0.4000)\n",
      "10 0.56875 tensor(0.6000)\n",
      "11 0.56875 tensor(0.4000)\n",
      "12 0.56875 tensor(0.4000)\n",
      "13 0.56875 tensor(0.6000)\n",
      "14 0.56875 tensor(0.2000)\n",
      "15 0.5687501 tensor(0.8000)\n",
      "16 0.5687501 tensor(0.4000)\n",
      "17 0.5687501 tensor(0.6000)\n",
      "18 0.56875 tensor(0.8000)\n",
      "19 0.56875 tensor(0.4000)\n",
      "20 0.56875 tensor(0.4000)\n",
      "21 0.5687501 tensor(0.8000)\n",
      "22 0.56875 tensor(0.8000)\n",
      "23 0.56875 tensor(0.8000)\n",
      "24 0.56875 tensor(0.6000)\n",
      "25 0.5687501 tensor(0.8000)\n",
      "26 0.56875 tensor(0.6000)\n",
      "27 0.5687501 tensor(0.6000)\n",
      "28 0.56875 tensor(0.6000)\n",
      "29 0.56875 tensor(0.2000)\n",
      "30 0.56875 tensor(0.4000)\n",
      "31 0.56875 tensor(0.6000)\n",
      "32 0.5687501 tensor(0.4000)\n",
      "33 0.56875 tensor(0.2000)\n",
      "34 0.56875 tensor(1.)\n",
      "35 0.56875 tensor(0.6000)\n",
      "36 0.56875 tensor(0.6000)\n",
      "37 0.56875 tensor(0.8000)\n",
      "38 0.56875 tensor(0.6000)\n",
      "39 0.56875 tensor(0.8000)\n",
      "40 0.5687501 tensor(0.2000)\n",
      "41 0.5687501 tensor(0.4000)\n",
      "42 0.5642361 tensor(0.8000)\n",
      "43 0.56875 tensor(0.6000)\n",
      "44 0.56875 tensor(0.6000)\n",
      "45 0.5687501 tensor(0.4000)\n",
      "46 0.56874996 tensor(0.4000)\n",
      "47 0.5687501 tensor(0.8000)\n",
      "48 0.56875 tensor(0.4000)\n",
      "49 0.5687501 tensor(0.8000)\n",
      "50 0.56875 tensor(0.4000)\n",
      "51 0.56874996 tensor(0.2000)\n",
      "52 0.56875 tensor(0.8000)\n",
      "53 0.56875 tensor(0.6000)\n",
      "54 0.56875 tensor(0.4000)\n",
      "55 0.56875 tensor(0.4000)\n",
      "56 0.56284726 tensor(0.6000)\n",
      "57 0.56875 tensor(0.2000)\n",
      "58 0.56875 tensor(0.6000)\n",
      "59 0.56875 tensor(0.8000)\n",
      "60 0.56875 tensor(0.8000)\n",
      "61 0.56875 tensor(0.4000)\n",
      "62 0.56875 tensor(1.)\n",
      "63 0.56875 tensor(0.6000)\n",
      "64 0.5687501 tensor(0.4000)\n",
      "65 0.5687501 tensor(0.6000)\n",
      "66 0.56875 tensor(0.2000)\n",
      "67 0.56875 tensor(0.8000)\n",
      "68 0.56875 tensor(0.8000)\n",
      "69 0.56875 tensor(0.4000)\n",
      "70 0.56875 tensor(0.6000)\n",
      "71 0.5687501 tensor(1.)\n",
      "72 0.56875 tensor(0.6000)\n",
      "73 0.56875 tensor(0.6000)\n",
      "74 0.56874996 tensor(0.8000)\n",
      "75 0.56875 tensor(0.8000)\n",
      "76 0.56874996 tensor(0.6000)\n",
      "77 0.56875 tensor(0.6000)\n",
      "78 0.56875 tensor(0.8000)\n",
      "79 0.56284726 tensor(0.8000)\n",
      "80 0.56354165 tensor(0.6000)\n",
      "81 0.56875 tensor(0.8000)\n",
      "82 0.5687501 tensor(0.8000)\n",
      "83 0.56875 tensor(0.2000)\n",
      "84 0.56875 tensor(0.6000)\n",
      "85 0.5687501 tensor(0.4000)\n",
      "86 0.5687501 tensor(0.4000)\n",
      "87 0.56875 tensor(0.6000)\n",
      "88 0.56875 tensor(0.4000)\n",
      "89 0.56875 tensor(0.4000)\n",
      "90 0.5652778 tensor(0.4000)\n",
      "91 0.56875 tensor(0.8000)\n",
      "92 0.5687501 tensor(0.4000)\n",
      "93 0.5642361 tensor(0.6000)\n",
      "94 0.5687501 tensor(0.4000)\n",
      "95 0.5687501 tensor(0.6000)\n",
      "96 0.56875 tensor(0.2000)\n",
      "97 0.56875 tensor(0.4000)\n",
      "98 0.56875 tensor(0.6000)\n",
      "99 0.56875 tensor(0.8000)\n"
     ]
    }
   ],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Create the model\n",
    "model = GraphSAGE(30, 32, 1)\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    acc_epoch = []\n",
    "    for data in loader:\n",
    "        data, target = data, data.y\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).squeeze()\n",
    "        \n",
    "        out = out.view(5,32)\n",
    "        mean_out = torch.mean(out,dim = 1)\n",
    "        \n",
    "        loss = criterion(mean_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (mean_out.round()==target).float().mean()\n",
    "        acc_epoch.append(acc)\n",
    "\n",
    "        # Print the model's output and the target for each batch\n",
    "        # print(f\"Output: {mean_out}, Target: {target}\")\n",
    "    wandb.log({\"loss\": loss.item(), \"accuracy\": np.mean(acc_epoch)})\n",
    "\n",
    "    print(epoch, np.mean(acc_epoch), acc_epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
