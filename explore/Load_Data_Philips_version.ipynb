{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data \n",
    "#!pip install scipy.io\n",
    "\n",
    "#import scipy.io\n",
    "#mat = scipy.io.loadmat('file.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install sciPy\n",
    "#!pip install h5py\n",
    "#import scipy.io\n",
    "#!pip install tables\n",
    "#!pip install matlab.engine\n",
    "#import matlab.engine\n",
    "#!pip install mat4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sklearn.metrics \n",
    "import h5py\n",
    "import tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "f = sio.loadmat('Data/sub000/After_remarks.mat')\n",
    "#print(f)\n",
    "#with open(Data/sub000/A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,28):\n",
    "#    print(f[\"After_remark\"][i][0][0][0],f[\"After_remark\"][i][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f[\"After_remark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is C4CE-7D5D\n",
      "\n",
      " Directory of c:\\Documents\\MsC_ITU\\2_Semester_MSc_ITU\\MiniProject_AML\\AML_project_gnn\\dataset\\EEG_Features\\DE\n",
      "\n",
      "04/18/2024  10:30 PM    <DIR>          .\n",
      "04/18/2024  10:30 PM    <DIR>          ..\n",
      "04/18/2024  10:29 PM         1,075,366 sub000.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub001.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub002.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub003.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub004.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub005.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub006.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub007.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub008.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub009.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub010.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub011.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub012.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub013.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub014.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub015.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub016.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub017.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub018.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub019.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub020.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub021.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub022.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub023.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub024.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub025.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub026.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub027.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub028.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub029.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub030.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub031.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub032.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub033.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub034.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub035.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub036.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub037.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub038.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub039.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub040.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub041.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub042.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub043.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub044.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub045.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub046.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub047.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub048.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub049.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub050.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub051.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub052.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub053.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub054.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub055.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub056.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub057.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub058.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub059.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub060.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub061.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub062.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub063.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub064.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub065.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub066.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub067.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub068.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub069.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub070.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub071.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub072.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub073.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub074.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub075.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub076.pkl.pkl\n",
      "04/18/2024  10:29 PM         1,075,366 sub077.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub078.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub079.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub080.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub081.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub082.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub083.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub084.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub085.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub086.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub087.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub088.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub089.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub090.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub091.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub092.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub093.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub094.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub095.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub096.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub097.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub098.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub099.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub100.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub101.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub102.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub103.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub104.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub105.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub106.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub107.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub108.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub109.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub110.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub111.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub112.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub113.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub114.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub115.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub116.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub117.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub118.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub119.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub120.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub121.pkl.pkl\n",
      "04/18/2024  10:30 PM         1,075,366 sub122.pkl.pkl\n",
      "             123 File(s)    132,270,018 bytes\n",
      "               2 Dir(s)  51,489,153,024 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir dataset\\EEG_Features\\DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_feature = pd.read_pickle(r'dataset\\EEG_Features\\DE\\sub000.pkl.pkl')\n",
    "PSD_feature = pd.read_pickle(r'dataset\\EEG_Features\\PSD\\sub000.pkl.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio\n",
    "#!pip3 install torchegg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torcheeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'After_remark'])\n"
     ]
    }
   ],
   "source": [
    "print(f.keys())\n",
    "movie_idx_list = []\n",
    "for i in range(0,28):\n",
    "    #print(f[\"After_remark\"][i][0][1],f[\"After_remark\"][i][0][2][0][0])#,f[\"After_remark\"][i][0][0])\n",
    "    movie_idx_list.append(f[\"After_remark\"][i][0][2][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desciption of array \n",
    "- 1) First array is movie \n",
    "- 2) Second is Electrodes \n",
    "- 3) Third is seconds\n",
    "- 4) Fourth is frequency bands\n",
    "- - 4.1: Delta band &rarr; 1-4 Hz, When people are sleeping, stage 2 NREM sleep, woemn seems to have more Delta-activity but is first apparent in early adulthood (30-40)\n",
    "- - 4.2: Theta band &rarr; 4-8 Hz, learning, memory and spatial Navigation, meditation can increase theta waves, can appear in sleep but not the deepest stages, active stages, the more movement the higher the theta state\n",
    "- - 4.3: Alpha band &rarr; 8-14 Hz, Two types with Type I being centered around occipital lobe, Type II happens during REM-sleep and is located at frontal-location in the brain, Not significantly linked to people with any major sleep disorder, chronic fatigue syndrome and major depression, alpha waves indicate idlenss, and mistakes are often happening when people are doing something automatically. \n",
    "- - 4.4: Beta band  &rarr; 14-30 Hz, can be seperated into three kinds of waves. Associated with waking consciousness, beta waves are accociated with the muscle contractions that happen in isotonic movements indicative of inhibitory cortical transmission mediated by gamma aminobutyric acid (GABA), the principal inhibitory neurotransmitter of the mammalian nervous system. Benzodiazepines, drugs that modulate GABAA receptors, induce beta waves in EEG recordings from humans and rats\n",
    "- - 4.5: Gamma Band &rarr; 30-47 Hz, Altered gamma activity has been observed in many mood and cognitive disorders such as Alzheimer's disease, epilepsy and schizophrenia For clinical relevance can be biomarker between unipolar and bipolar disorder, decrease Gamma activity is obser in schizophrenia  \n",
    "\n",
    "# Describtion of metrics: \n",
    "- 1) Differential entropy: $DE = \\frac{1}{2}ln(2\\pi e \\sigma^2), Shannon entropy on continuous functions, assumes normal distribution. \n",
    "- 2) Power Spectral density: $PSD =E[X^2]$, exists for stationary processes, power of a signal or time series distruted over frequency \n",
    "- 3) Where x is the EEG signal and $\\sigma$ is the variance of the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "f = sio.loadmat('Data/sub000/After_remarks.mat')\n",
    "clip_seen = f[\"After_remark\"][0][0][2][0][0]\n",
    "\n",
    "number = f[\"After_remark\"][2][0][2][0][0]\n",
    "\n",
    "number = torch.from_numpy(np.asarray(number))\n",
    "#number = Gamma_extract.type(torch.Tensor)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 32, 30, 5)\n",
      "(28, 32, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "# First array is movie \n",
    "# Second is Electrodes \n",
    "# Third is seconds\n",
    "# Fourth is frequency bands\n",
    "import torch\n",
    "from torcheeg import transforms\n",
    "from torcheeg.datasets.constants import DEAP_ADJACENCY_MATRIX\n",
    "from torcheeg.transforms.pyg import ToG\n",
    "from torcheeg.utils import plot_3d_tensor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(DE_feature.shape)\n",
    "\n",
    "print(PSD_feature.shape)\n",
    "\n",
    "\n",
    "\n",
    "DE_feature[0][0].shape\n",
    "\n",
    "Gamma_extract = DE_feature[0,:,:,-1]\n",
    "#print(Gamma_extract)\n",
    "\n",
    "#img = plot_3d_tensor(torch.tensor(dataset[0][][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Should we transform???\n",
    "\n",
    "#t = transforms.ToGrid(Gamma_extract)\n",
    "#t = transforms.BaselineRemoval()\n",
    "\n",
    "#eeg = t(eeg=Gamma_extract, baseline=np.random.randn(32, 30))['eeg']\n",
    "#transformed_eeg = transforms.BandDifferentialEntropy()(eeg=eeg)\n",
    "#print(transformed_eeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([30, 32, 5])\n",
      "1\n",
      "torch.Size([30, 32, 5])\n",
      "2\n",
      "torch.Size([30, 32, 5])\n",
      "3\n",
      "torch.Size([30, 32, 5])\n",
      "4\n",
      "torch.Size([30, 32, 5])\n",
      "5\n",
      "torch.Size([30, 32, 5])\n",
      "6\n",
      "torch.Size([30, 32, 5])\n",
      "7\n",
      "torch.Size([30, 32, 5])\n",
      "8\n",
      "torch.Size([30, 32, 5])\n",
      "9\n",
      "torch.Size([30, 32, 5])\n",
      "10\n",
      "torch.Size([30, 32, 5])\n",
      "11\n",
      "torch.Size([30, 32, 5])\n",
      "12\n",
      "torch.Size([30, 32, 5])\n",
      "13\n",
      "torch.Size([30, 32, 5])\n",
      "14\n",
      "torch.Size([30, 32, 5])\n",
      "15\n",
      "torch.Size([30, 32, 5])\n",
      "16\n",
      "torch.Size([30, 32, 5])\n",
      "17\n",
      "torch.Size([30, 32, 5])\n",
      "18\n",
      "torch.Size([30, 32, 5])\n",
      "19\n",
      "torch.Size([30, 32, 5])\n",
      "20\n",
      "torch.Size([30, 32, 5])\n",
      "21\n",
      "torch.Size([30, 32, 5])\n",
      "22\n",
      "torch.Size([30, 32, 5])\n",
      "23\n",
      "torch.Size([30, 32, 5])\n",
      "24\n",
      "torch.Size([30, 32, 5])\n",
      "25\n",
      "torch.Size([30, 32, 5])\n",
      "26\n",
      "torch.Size([30, 32, 5])\n",
      "27\n",
      "torch.Size([30, 32, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for clip in range(0,28):\n",
    "    print(clip)\n",
    "    Gamma_extract = DE_feature[clip:clip+1,:,:,:]\n",
    "    Gamma_extract = np.moveaxis(Gamma_extract, 1, 2)\n",
    "\n",
    "    Gamma_extract = torch.from_numpy(Gamma_extract)\n",
    "    Gamma_extract = Gamma_extract.type(torch.Tensor)\n",
    "\n",
    "    print(Gamma_extract[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#egg[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 32, 5])\n",
      "torch.Size([30, 5])\n",
      "11\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (30) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[286], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([number], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#print(target.shape)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (30) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "from torcheeg.models import DGCNN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "f = sio.loadmat('Data/sub000/After_remarks.mat')\n",
    "\n",
    "model = DGCNN(in_channels = 5,  num_electrodes=32, hid_channels = 1, num_classes=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()  # Choose an appropriate loss function\n",
    "\n",
    "# Training loop\n",
    "um_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for clip in range(0,28):\n",
    "        # Prepare input data\n",
    "        Gamma_extract = DE_feature[clip:clip+1,:,:,:]\n",
    "        Gamma_extract = np.moveaxis(Gamma_extract, 1, 2)\n",
    "        inputs = torch.from_numpy(Gamma_extract).float()\n",
    "        print(inputs.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs[0])\n",
    "        print(outputs.shape)\n",
    "        # Prepare target data\n",
    "        number = f[\"After_remark\"][clip][0][2][0][0]\n",
    "        print(number)\n",
    "        #print(number)\n",
    "        target = torch.tensor([number], dtype=torch.long)\n",
    "        #print(target.shape)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print loss or other metrics if desired\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheeg.models.cnn import TSCeption\n",
    "model = TSCeption(num_classes=9,\n",
    "                  num_electrodes=32,\n",
    "                  sampling_rate=128,\n",
    "                  num_T=15,\n",
    "                  num_S=15,\n",
    "                  hid_channels=32,\n",
    "                  dropout=0.5)\n",
    "pred = model(eeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
